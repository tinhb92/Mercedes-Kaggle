{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# preprocessing/decomposition\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis, KernelPCA, TruncatedSVD\n",
    "\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# supportive models\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# feature selection (from supportive model)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "seed = 42 \n",
    "\n",
    "import os \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape: (8418, 373)\n"
     ]
    }
   ],
   "source": [
    "INPUT = '/media/tin/DATA/Kaggle/Mercedes/Input/'\n",
    "OUTPUT = '/media/tin/DATA/Kaggle/Mercedes/Output/'\n",
    "\n",
    "train = pd.read_csv(os.path.join(INPUT, 'train_section.csv'))\n",
    "test = pd.read_csv(os.path.join(INPUT, 'test_section.csv'))\n",
    "\n",
    "train = train.drop('X11', axis=1)\n",
    "train = train.drop('X93', axis=1)\n",
    "train = train.drop('X107', axis=1)\n",
    "train = train.drop('X233', axis=1)\n",
    "train = train.drop('X235', axis=1)\n",
    "train = train.drop('X268', axis=1)\n",
    "\n",
    "test = test.drop('X11', axis=1)\n",
    "test = test.drop('X93', axis=1)\n",
    "test = test.drop('X107', axis=1)\n",
    "test = test.drop('X233', axis=1)\n",
    "test = test.drop('X235', axis=1)\n",
    "test = test.drop('X268', axis=1)\n",
    "\n",
    "# save IDs for submission\n",
    "id_test = test['ID'].copy()\n",
    "\n",
    "# glue datasets together\n",
    "total = pd.concat([train, test], axis=0)\n",
    "print('initial shape: {}'.format(total.shape))\n",
    "\n",
    "# binary indexes for train/test set split\n",
    "is_train = ~total.y.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in ['X0', 'group']:\n",
    "    total[col] = total[col].astype('object')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# find all categorical features\n",
    "cf = total.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8', 'X0', 'group'], dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cf.append(pd.Index(['X0', 'group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh-encoded shape: (8418, 216)\n",
      "appended-encoded shape: (8418, 580)\n",
      "\n",
      "Train shape: (4209, 579)\n",
      "Test shape: (4209, 578)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# make one-hot-encoding convenient way - pandas.get_dummies(df) function\n",
    "dummies = pd.get_dummies(\n",
    "    total[cf],\n",
    "    drop_first=False # you can set it = True to ommit multicollinearity (crucial for linear models)\n",
    ")\n",
    "\n",
    "print('oh-encoded shape: {}'.format(dummies.shape))\n",
    "\n",
    "# get rid of old columns and append them encoded\n",
    "total = pd.concat(\n",
    "    [\n",
    "        total.drop(cf, axis=1), # drop old\n",
    "        dummies # append them one-hot-encoded\n",
    "    ],\n",
    "    axis=1 # column-wise\n",
    ")\n",
    "\n",
    "print('appended-encoded shape: {}'.format(total.shape))\n",
    "\n",
    "# recreate train/test again, now with dropped ID column\n",
    "train, test = total[is_train].drop(['ID'], axis=1), total[~is_train].drop(['ID', 'y'], axis=1)\n",
    "\n",
    "# drop redundant objects\n",
    "del total\n",
    "\n",
    "# check shape\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/tin/DATA/newbigtf/lib/python3.5/site-packages/sklearn/decomposition/fastica_.py:116: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    }
   ],
   "source": [
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (4209, 653)\n",
      "Test shape: (4209, 652)\n"
     ]
    }
   ],
   "source": [
    "# create augmentation by feature importances as additional features\n",
    "t = train['y']\n",
    "tr = train.drop(['y'], axis=1)\n",
    "\n",
    "# Tree-based estimators can be used to compute feature importances\n",
    "clf = GradientBoostingRegressor(\n",
    "                max_depth=4, \n",
    "                learning_rate=0.005, \n",
    "                random_state=seed, \n",
    "                subsample=0.95, \n",
    "                n_estimators=200\n",
    ")\n",
    "\n",
    "# fit regressor\n",
    "clf.fit(tr, t)\n",
    "\n",
    "# df to hold feature importances\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = tr.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "\n",
    "# select best features\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_reduced = model.transform(tr)\n",
    "\n",
    "test_reduced = model.transform(test.copy())\n",
    "\n",
    "# dataset augmentation\n",
    "train = pd.concat([train, pd.DataFrame(train_reduced)], axis=1)\n",
    "test = pd.concat([test, pd.DataFrame(test_reduced)], axis=1)\n",
    "\n",
    "# check new shape\n",
    "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(INPUT,'train_engineer.csv'), index = False)\n",
    "test.to_csv(os.path.join(INPUT,'test_engineer.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
